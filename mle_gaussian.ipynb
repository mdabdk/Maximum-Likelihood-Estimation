{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation for Normal Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates finding the maximum likelihood estimates of the mean $\\mu$ and the standard deviation $\\sigma$ for a Normal random variable $X$. The probability density function of $X$ is:\n",
    "\n",
    "$$f_X(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left({-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\right)$$\n",
    "\n",
    "Suppose that you observe 500 independent and identically distributed (i.i.d) samples of $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 500\n",
      "Variance: 9.628891944885254\n",
      "Mean: 2.014725923538208\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "mu = 2\n",
    "sigma = 3\n",
    "num_samples = 500\n",
    "\n",
    "X = torch.randn((num_samples,),requires_grad = False)\n",
    "X = sigma*X + mu\n",
    "\n",
    "print('Number of samples: {}\\nVariance: {}\\nMean: {}'.format(X.shape[0],*torch.var_mean(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The independence assumption is valid if observing one of the samples does not give you information about the other samples, while the identically distributed assumption makes sense if these observations originated from the same underlying random experiment. Therefore, the dataset $\\mathcal{D}$ is:\n",
    "\n",
    "$$\\mathcal{D} = \\{X = x_1,X = x_2,...,X = x_{500}\\}$$\n",
    "\n",
    "The likelihood function is therefore:\n",
    "\n",
    "$$ p(\\mathcal{D};\\mu,\\sigma^2) = \\prod_{i=1}^{500}f_X(x_i;\\mu_i,\\sigma_{i}^2) = \\prod_{i=1}^{500} \\frac{1}{\\sqrt{2\\pi\\sigma_{i}^2}} \\exp\\left({-\\frac{(x_i-\\mu_i)^2}{2\\sigma_{i}^2}}\\right) $$\n",
    "\n",
    "Since the samples are identically distributed:\n",
    "\n",
    "$$\n",
    "\\mu_1 = \\mu_2 = ... = \\mu_{500} = \\mu \\\\\n",
    "\\sigma_{1}^2 = \\sigma_{2}^2 = ... = \\sigma_{500}^2 = \\sigma^2\n",
    "$$\n",
    "\n",
    "And the log-likelihood function is:\n",
    "\n",
    "$$ \\ln(p(\\mathcal{D};\\mu,\\sigma^2)) = \\sum_{i=1}^{500} \\ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma_{i}^2}}\\right) + \\ln\\left(\\exp\\left({-\\frac{(x_i-\\mu_i)^2}{2\\sigma_{i}^2}}\\right)\\right)$$\n",
    "\n",
    "$$= -\\left(\\sum_{i=1}^{500} \\ln\\left(\\sqrt{2\\pi\\sigma_{i}^2}\\right) + \\sum_{i=1}^{500} \\frac{(x_i-\\mu_i)^2}{2\\sigma_{i}^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(282913.4688)\n"
     ]
    }
   ],
   "source": [
    "# Negative log-likelihood of X\n",
    "\n",
    "from math import pi\n",
    "\n",
    "def normal_NLLL(X,mu,sigma):\n",
    "    first_term = torch.sum(torch.log(torch.sqrt(2*pi*torch.pow(sigma,2))))\n",
    "    second_term = torch.sum(torch.div(torch.pow(X-mu,2),2*torch.pow(sigma,2)))\n",
    "    return (first_term + second_term)\n",
    "\n",
    "# track gradients of NLLL with respect to parameters\n",
    "\n",
    "mu = torch.randn((num_samples,))\n",
    "sigma = torch.randn((num_samples,))\n",
    "\n",
    "NLLL = normal_NLLL(X,mu,sigma)\n",
    "\n",
    "print(NLLL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the maximum likelihood estimates of $\\mu$ and $\\sigma^2$ are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\DeclareMathOperator*{\\argmin}{\\arg\\!\\min}\n",
    "\\hat{\\mu} = \\argmin_{\\mu}{\\left(-\\ln\\left(p\\left(\\mathcal{D};\\mu,\\sigma^2\\right)\\right)\\right)} =  \\argmin_{\\mu}\\sum_{i=1}^{500} \\frac{(x_i-\\mu)^2}{2\\sigma^2}\\\\\n",
    "\\hat{\\sigma}^2 = \\argmin_{\\sigma^2}{\\left(-\\ln\\left(p\\left(\\mathcal{D};\\mu,\\sigma^2\\right)\\right)\\right)} = \\argmin_{\\sigma^2} \\left(\\sum_{i=1}^{500}\\ln\\left(\\sqrt{2\\pi\\sigma^2}\\right) + \\sum_{i=1}^{500} \\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "$\\hat{\\mu}$ and $\\hat{\\sigma}^2$ can be computed analytically. First for $\\hat{\\mu}$:\n",
    "\n",
    "$$- \\frac{\\partial\\ln\\left(p\\left(\\mathcal{D};\\mu,\\sigma^2\\right)\\right)}{\\partial\\mu} = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{500} -2(x_i-\\mu) = -\\frac{1}{\\sigma^2} \\sum_{i=1}^{500} x_i-\\mu = 0 $$\n",
    "\n",
    "Solving for $\\mu$:\n",
    "\n",
    "$$\\hat{\\mu} = \\frac{\\sum_{i=1}^{500} x_i}{500}$$\n",
    "\n",
    "Which is just the sample mean. Similarly, for the standard deviation:\n",
    "\n",
    "$$\n",
    "-\\frac{\\partial\\ln\\left(p\\left(\\mathcal{D};\\mu,\\sigma^2\\right)\\right)}{\\partial\\sigma^2} = \\frac{500}{2\\sigma^2} - \\frac{1}{2\\sigma^4} \\sum_{i=1}^{500} (x_i-\\mu)^2 = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{500} \\sum_{i=1}^{500} (x_i-\\mu)^2\n",
    "$$\n",
    "\n",
    "Which is just the sample variance. However, this is a biased estimator of the $\\sigma^2$. Instead, the sample variance can be computed as follows:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{500-1} \\sum_{i=1}^{500} (x_i-\\mu)^2\n",
    "$$\n",
    "\n",
    "This modification is called [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction). The maximum likelihood estimates of $\\mu$ and $\\sigma^2$ can then be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0147)\n"
     ]
    }
   ],
   "source": [
    "# sample mean\n",
    "\n",
    "mu_hat = torch.sum(X)/X.shape[0]\n",
    "\n",
    "print(mu_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.6289)\n"
     ]
    }
   ],
   "source": [
    "# sample variance\n",
    "\n",
    "sigma_squared_hat = torch.sum(torch.pow(X-mu_hat,2))/(X.shape[0]-1)\n",
    "\n",
    "print(sigma_squared_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the maximum likelihood estimates of $\\mu$ and $\\sigma^2$ can be computed using gradient descent. Gradient descent is an iterative algorithm based on the following general methodology:\n",
    "1. Guess initial values for $\\mu$ and $\\sigma$.\n",
    "2. Use the rate of change (gradient) of the negative log-likeihood function at these initial values of $\\mu$ and $\\sigma$ to find new values for $\\mu$ and $\\sigma$ that minimize the negative log-likelihood function. More precisely, if $\\mu_0$ and $\\sigma_0$ are the initial values, then use:\n",
    "\n",
    "$$\n",
    "-\\left.\\frac{\\partial\\ln\\left(p\\left(\\mathcal{D};\\mu,\\sigma^2\\right)\\right)}{\\partial\\mu} \\right\\rvert_{\\mu = \\mu_0} \\\\\n",
    "-\\left.\\frac{\\partial\\ln\\left(p\\left(\\mathcal{D};\\mu,\\sigma^2\\right)\\right)}{\\partial\\sigma} \\right\\rvert_{\\sigma = \\sigma_0}\n",
    "$$\n",
    "\n",
    "3. Repeat steps 1 and 2 until convergence.\n",
    "\n",
    "The following explanation for how to choose $f(\\cdot)$ is adapted from [here](https://eli.thegreenplace.net/2016/understanding-gradient-descent/). Consider the simple function $f(x) = x^2$. The value of $f(x)$ decreases when x is negative and increasing, while its value increases when x is positive and increasing. Suppose that you do not know where the minimum of $f(x)$ is. Let us guess first that the minimum is at $x = -2$. The derivative of $f(x)$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute new estimates of $\\mu$ and $\\sigma$ using the following update rules:\n",
    "\n",
    "$$\n",
    "\\mu_{n+1} = \\mu_n + \\eta f(\\mathcal{D},\\mu_n,\\sigma_n) \\\\\n",
    "\\sigma_{n+1} = \\sigma_n + \\eta f(\\mathcal{D},\\mu_n,\\sigma_n)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $\\mu_{n+1}$ and $\\sigma_{n+1}$ are the new estimates of $\\mu$ and $\\sigma$.\n",
    "* $\\mu_n$ and $\\sigma_n$ are the current estimates of $\\mu$ and $\\sigma$.\n",
    "* $f(\\cdot)$ is some function of the dataset $\\mathcal{D}$ and the current estimates of $\\mu$ and $\\sigma$.\n",
    "* $\\eta$ is called the _learning rate_ which controls the magnitude of $f(\\cdot)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
